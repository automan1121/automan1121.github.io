<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>零碎知识点</title>
      <link href="/2019/03/27/%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2019/03/27/%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h1 id="零碎知识点"><a href="#零碎知识点" class="headerlink" title="零碎知识点"></a><font color="#2674BA">零碎知识点</font></h1><h3 id="Q1-数据归一化、标准化"><a href="#Q1-数据归一化、标准化" class="headerlink" title="Q1:数据归一化、标准化  "></a><font color="#F77A0B">Q1:数据归一化、标准化  </font></h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:left">归一化</th><th style="text-align:left">标准化</th></tr></thead><tbody><tr><td style="text-align:center">特点</td><td style="text-align:left">数据缩放到0-1区间<br>不同维度的特征对目标函数的影响权重一致                       <br>由最大最小值决定<br>改变了原数据分布(梯度下降等高线由椭圆变成圆)</td><td style="text-align:left">消除量纲<br>不同维度的特征具有可比性 <br>由数据的整体分布决定<br>不改变原数据分布</td></tr><tr><td style="text-align:center">常用方法</td><td style="text-align:left">$x^* = \frac{x-min}{max-min} $</td><td style="text-align:left">$x^* = \frac{x-\bar x}{\sigma} $</td></tr><tr><td style="text-align:center">使用场景</td><td style="text-align:left">输出结果范围有要求<br>数据较稳定，不存在极端的最大最小值</td><td style="text-align:left">数据存在异常值和较多噪音</td></tr></tbody></table><p>哪些模型需要归一化、哪些不需要？</p><ul><li>使用梯度下降优化的模型最好归一化，归一化后损失函数的等高线接近圆形，收敛速度加快。</li><li>树型模型不需要归一化，分割点的选取与特征的绝对值大小无关。</li></ul><a id="more"></a><h3 id="Q2-过拟合"><a href="#Q2-过拟合" class="headerlink" title=" Q2:过拟合  "></a><font color="#F77A0B"> Q2:过拟合  </font></h3><p><a><strong>什么叫过拟合？</strong></a></p><ul><li>模型在训练数据表现好但在验证数据表现差的现象。（<strong>学得好但是考不好</strong>）<br><a><strong>过拟合造成的原因以及如何判断过拟合？</strong></a><br><strong>原因：</strong></li><li>1.训练数据与验证数据分布差异大。（训练数据噪声大、训练样本小等）</li><li>2.模型复杂度过高。（参数过多、参数值过大等）<br><strong>极端示例：</strong></li><li>1.训练样本只需要学习<strong>1个</strong>知识点，验证样本包含<strong>100个</strong>知识点。（学到的只是很小一部分知识）</li><li>2.训练样本学习<strong>A</strong>知识点，验证样本考察<strong>BCD</strong>知识点。（学的都是不考的）</li><li>3.参数过多。（极端情况下，无穷个参数一定能拟合任何曲线）</li><li>4.参数值过大。（希望<strong>拟合所有的样本点</strong>，拟合曲线的波动大，当x小而需要y大时则w大）<br><strong>判断：</strong></li><li>模型的训练误差不断降低，测试误差不断上升。<br><a><strong>怎么防止过拟合？</strong></a></li><li>early stopping、正则化、扩大数据集、dropout等。<h3 id="Q3-正则化"><a href="#Q3-正则化" class="headerlink" title="Q3:正则化  "></a><font color="#F77A0B">Q3:正则化  </font></h3><a><strong>什么叫正则化？常见的正则化有哪些？</strong></a></li><li>为了防止过拟合，在损失函数的基础上增加一个<font color="#F77A0B"><strong>参数的惩罚项</strong></font>（正则），使训练误差最小化时，模型的复杂度尽可能低，保证模型的泛化能力。</li><li>常见的有L0、L1、L2。L0、L1实现<font color="#F77A0B"><strong>稀疏</strong></font>（稀疏可特征选择、提高可解释性），L2实现<font color="#F77A0B"><strong>平滑</strong></font>（权值衰减，防止过拟合）。<ul><li>L0：向量中非0的个数，最小化L0即希望参数矩阵w中大部分元素都为0。</li><li>L1：向量中的各元素绝对值之和$||w||_1$，最小化L1即参数矩阵w中大部分元素都为0（$||w||_1$在0不可导，参数以固定速率下降到0），L1为L0的近似解。</li><li>L2：向量中的各元素平方之和$||w||_2$，最小化L2即参数矩阵w中大部分元素都接近0（$||w||_1$在0可导，参数约接近0下降速率越慢，越平滑）。</li></ul></li></ul><p><a><strong>正则化为什么能增加模型的泛化能力，防止过拟合？</strong></a></p><ul><li>造成过拟合原因第2点：模型复杂度高（参数多、数值大）。正则化可以通过约束参数的数目与大小，来解决由参数引起的过拟合。</li><li>“欧姆剃刀”：训练误差最小的同时模型的复杂度越低，模型的泛化能力越强。<br><a><strong>L1与L2差异？</strong></a></li><li>L1、L2都可以防止过拟合。</li><li>L1-稀疏、L2-平滑。<br><a><strong>L1、L2的使用场景？什么时候L1优于L2？什么时候使用L1+L2?</strong></a></li><li>关注稀疏性、特征选择、可解释性时选择L1。</li><li>其他场景一般选择L2。</li><li>Elatic Net问题，平衡稀疏与光滑的问题。<h3 id="Q4-聚类、分类、回归等的评价指标以及使用场景？"><a href="#Q4-聚类、分类、回归等的评价指标以及使用场景？" class="headerlink" title="Q4:聚类、分类、回归等的评价指标以及使用场景？  "></a><font color="#F77A0B">Q4:聚类、分类、回归等的评价指标以及使用场景？  </font></h3></li><li>聚类用的比较少，只用过k-means，常用的评价指标：依赖业务知识、计算类内的平均距离（<a href="https://zhuanlan.zhihu.com/p/53840697" target="_blank" rel="noopener">附录:知乎回答</a>）</li><li>分类常用：Precision(精确率：$\frac{预测label为1且真实label为1}{预测label为1} $$)、Recall(召回率：$$\frac{预测label为1且真实label为1}{真实label为1}$$)、Accuracy(准确率：$$\frac{预测label与真实label相同}{样本总数}$$)、F1（$$\frac{2P*R}{P+R}$）、ROC、AUC<br><div align="center"><strong>ROC曲线</strong><br><div align="center"><img src="/img/roc.jpg" alt=""></div></div></li><li><strong>混淆矩阵</strong></li></ul><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">实际1（Positive）</th><th style="text-align:center">实际0（Negative）</th></tr></thead><tbody><tr><td style="text-align:center"><strong>预测1（Positive）</strong></td><td style="text-align:center">（True Positive）TP</td><td style="text-align:center">（False Positive）FP</td></tr><tr><td style="text-align:center"><strong>预测0（Negative）</strong></td><td style="text-align:center">（False Negative）FN</td><td style="text-align:center">（True Negative）TN</td></tr></tbody></table><p><strong>ROC横坐标（假阳率-实际没病误判有病）：</strong> $FPR = \frac{FP}{FP+TN}$<br><strong>ROC纵坐标（真阳率-实际有病判别有病）：</strong> $TPR = \frac{TP}{TP+FN}$<br><strong>ROC坐标点（0，0）：</strong>实际没病误判有病=0、实际有病判别有病=0  ==&gt; 都预测0（无病）<br><strong>ROC坐标点（0，1）：</strong>实际没病误判有病=0、实际有病判别有病=1  ==&gt;全对<br><strong>ROC坐标点（1，0）：</strong>实际没病误判有病=1、实际有病判别有病=0 ==&gt; 全错<br><strong>ROC坐标点（1，1）：</strong>实际没病误判有病=1、实际有病判别有病=1  ==&gt; 都预测为1（有病）<br><strong>ROC曲线约靠近左上角，分类性能越好</strong></p><p><strong>AUC：</strong>ROC曲线下的面积、越大分类性能越好</p><h3 id="Q5-数据不平衡的处理方式？"><a href="#Q5-数据不平衡的处理方式？" class="headerlink" title="Q5:数据不平衡的处理方式？  "></a><font color="#F77A0B">Q5:数据不平衡的处理方式？  </font></h3><h3 id="Q6-优化算法，随机梯度以及各种变式？"><a href="#Q6-优化算法，随机梯度以及各种变式？" class="headerlink" title="Q6:优化算法，随机梯度以及各种变式？  "></a><font color="#F77A0B">Q6:优化算法，随机梯度以及各种变式？  </font></h3><h3 id="Q7-特征选择？"><a href="#Q7-特征选择？" class="headerlink" title="Q7:特征选择？  "></a><font color="#F77A0B">Q7:特征选择？  </font></h3><h3 id="Q8-常见模型调参？"><a href="#Q8-常见模型调参？" class="headerlink" title="Q8:常见模型调参？  "></a><font color="#F77A0B">Q8:常见模型调参？  </font></h3><h3 id="Q9-特征为什么要离散化以及常见方法？"><a href="#Q9-特征为什么要离散化以及常见方法？" class="headerlink" title="Q9:特征为什么要离散化以及常见方法？  "></a><font color="#F77A0B">Q9:特征为什么要离散化以及常见方法？  </font></h3><h3 id="Q10-熵、交叉熵、JS散度？"><a href="#Q10-熵、交叉熵、JS散度？" class="headerlink" title="Q10:熵、交叉熵、JS散度？  "></a><font color="#F77A0B">Q10:熵、交叉熵、JS散度？  </font></h3>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归</title>
      <link href="/2019/03/27/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2019/03/27/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>逻辑回归</title>
      <link href="/2019/03/27/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2019/03/27/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2019年计划</title>
      <link href="/2019/01/20/2019%E5%B9%B4%E8%AE%A1%E5%88%92/"/>
      <url>/2019/01/20/2019%E5%B9%B4%E8%AE%A1%E5%88%92/</url>
      
        <content type="html"><![CDATA[<ul><li>1.</li><li>2.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 2019 </tag>
            
            <tag> 计划 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于本站</title>
      <link href="/2019/01/20/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99/"/>
      <url>/2019/01/20/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<p>本站域名 <a href="http://babylls.com" target="_blank" rel="noopener">http://babylls.com</a> 站内分享一些机器学习、深度学习、数据挖掘、编程语言相关的文章。学识尚浅，文中难免会有错误，如您发现，请邮件(<a href="mailto:babylls@163.com" target="_blank" rel="noopener">babylls@163.com</a>)告知，同时也欢迎与我交流。</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
